{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["_I_56LqwWp9f","osgkmyzOXH6k","K7XX7JkBuZuT"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Step 2 of CAFFE (EXAMPLE)\n","The second step of CAFFE involves an human-in-the-loop approach to obtain responses for the prompts generated as test data in Step 1 by the LLM under test. Hence, this step is highly dependent on the users' necessities. However, we provide an example of how we used Step 2 of CAFFE in our experiments as a guideline for users.\n","\n","\n","-----------------\n"],"metadata":{"id":"_I_56LqwWp9f"}},{"cell_type":"markdown","source":["## Example with GPT APIs"],"metadata":{"id":"osgkmyzOXH6k"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"AGp4L8TwWYtS"},"outputs":[],"source":["import os\n","import time\n","import pandas as pd\n","from tqdm import tqdm\n","from openai import OpenAI\n","from google.colab import userdata\n","import sys\n","\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n","client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"]},{"cell_type":"code","source":["def gpt_reply(prompt: str, client: OpenAI, model: str = \"gpt-4o-mini\") -> str:\n","    try:\n","        res = client.chat.completions.create(\n","            model=model,\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","        )\n","        return res.choices[0].message.content.strip()\n","    except Exception as e:\n","        print(f\"Error with prompt «{prompt[:50]}…»: {e}\")\n","        return \"\"\n"],"metadata":{"id":"hzJLb8JyXNuE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_responses(filename):\n","    # === Load API Key ===\n","    load_dotenv()\n","    api_key = os.getenv(\"OPENAI_API_KEY\")\n","    if not api_key:\n","        raise ValueError(\"OPENAI_API_KEY not set in .env file\")\n","    client = OpenAI(api_key=api_key)\n","\n","    # === File Paths ===\n","    input_csv = filename\n","    output_csv = f\"Responses_{filename}\"\n","    backup_csv = f\"Backup_{filename}\"\n","\n","    # === Load Data ===\n","    df = pd.read_csv(input_csv)\n","\n","    required = {\"sentence_1\", \"sentence_2\"}\n","    missing = required - set(df.columns)\n","    if missing:\n","        raise ValueError(f\"Missing required columns: {missing}\")\n","\n","    for col in (\"response_1\", \"response_2\"):\n","        if col not in df.columns:\n","            df[col] = \"\"\n","\n","    # === Generate Responses ===\n","    df[\"response_1\"] = df[\"response_1\"].fillna(\"\").astype(str)\n","    df[\"response_2\"] = df[\"response_2\"].fillna(\"\").astype(str)\n","\n","    df_missing = df[\n","        (df[\"response_1\"].str.strip() == \"\") |\n","        (df[\"response_2\"].str.strip() == \"\")\n","    ]\n","\n","    print(f\"Rows to process: {len(df_missing)}\")\n","    first_write = not os.path.exists(output_csv)\n","    print(f\"Missing response_1: {(df['response_1'] == '').sum()}\")\n","    print(f\"Missing response_2: {(df['response_2'] == '').sum()}\")\n","\n","    total_missing = (df[\"response_1\"] == \"\").sum() + (df[\"response_2\"] == \"\").sum()\n","\n","    with tqdm(total=total_missing, desc=\"Generating Responses\") as pbar:\n","        for idx, row in df.iterrows():\n","            updated = False\n","\n","            if row[\"response_1\"] == \"\":\n","                df.at[idx, \"response_1\"] = gpt_reply(row[\"sentence_1\"], client)\n","                updated = True\n","                pbar.update(1)\n","\n","            if row[\"response_2\"] == \"\":\n","                df.at[idx, \"response_2\"] = gpt_reply(row[\"sentence_2\"], client)\n","                updated = True\n","                pbar.update(1)\n","\n","            if updated:\n","                df.iloc[[idx]].to_csv(output_csv, mode=\"a\", header=first_write, index=False)\n","                first_write = False\n","                time.sleep(1)\n","\n","    print(f\"Done. Responses saved to: {output_csv}\")\n"],"metadata":{"id":"ErwFqA4QXQd0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = \"\" #name of the file containing the test data generated by Step 1\n","\n","get_responses(filename)"],"metadata":{"id":"keaHm-eIXXtN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Example with LLAMA on LMSTUDIO"],"metadata":{"id":"K7XX7JkBuZuT"}},{"cell_type":"code","source":["import os\n","import time\n","import sys\n","import pandas as pd\n","import requests\n","from tqdm import tqdm\n","from dotenv import load_dotenv"],"metadata":{"id":"-aQSulEEuY7C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === LLM Local Chat Completion ===\n","def llama_reply(prompt: str, api_url: str, model: str = \"local-model\") -> str:\n","    try:\n","        response = requests.post(\n","            api_url,\n","            headers={\"Content-Type\": \"application/json\"},\n","            json={\n","                \"model\": model,\n","                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n","                \"temperature\": 0.5,\n","            }\n","        )\n","        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n","    except Exception as e:\n","        print(f\"Error with prompt «{prompt[:50]}…»: {e}\")\n","        return \"\""],"metadata":{"id":"1rFvqRNAuiTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_responses(filename):\n","    # === Load .env for API URL ===\n","    load_dotenv()\n","    api_url = \"lmstudio_local_URL\"\n","    model_name = \"local-model\"\n","\n","    # === File Paths ===\n","    input_csv = filename\n","    output_csv = f\"Responses_{filename}\"\n","    backup_csv = f\"Backup_{filename}\"\n","\n","    # === Load Data ===\n","    df = pd.read_csv(input_csv)\n","    required = {\"sentence_1\", \"sentence_2\"}\n","    missing = required - set(df.columns)\n","    if missing:\n","        raise ValueError(f\"Missing required columns: {missing}\")\n","\n","    for col in (\"response_1\", \"response_2\"):\n","        if col not in df.columns:\n","            df[col] = \"\"\n","\n","    # Backup original\n","    df.to_csv(backup_csv, index=False)\n","\n","    # === Clean up existing responses ===\n","    df[\"response_1\"] = df[\"response_1\"].fillna(\"\").astype(str)\n","    df[\"response_2\"] = df[\"response_2\"].fillna(\"\").astype(str)\n","\n","    df_missing = df[\n","        (df[\"response_1\"].str.strip() == \"\") |\n","        (df[\"response_2\"].str.strip() == \"\")\n","    ]\n","\n","    print(f\"Rows to process: {len(df_missing)}\")\n","    first_write = not os.path.exists(output_csv)\n","    print(f\"Missing response_1: {(df['response_1'] == '').sum()}\")\n","    print(f\"Missing response_2: {(df['response_2'] == '').sum()}\")\n","\n","    total_missing = (df[\"response_1\"] == \"\").sum() + (df[\"response_2\"] == \"\").sum()\n","\n","    with tqdm(total=total_missing, desc=\"Generating Responses\") as pbar:\n","        for idx, row in df.iterrows():\n","            updated = False\n","\n","            if row[\"response_1\"] == \"\":\n","                df.at[idx, \"response_1\"] = llama_reply(row[\"sentence_1\"], api_url, model=model_name)\n","                updated = True\n","                pbar.update(1)\n","\n","            if row[\"response_2\"] == \"\":\n","                df.at[idx, \"response_2\"] = llama_reply(row[\"sentence_2\"], api_url, model=model_name)\n","                updated = True\n","                pbar.update(1)\n","\n","            if updated:\n","                df.iloc[[idx]].to_csv(output_csv, mode=\"a\", header=first_write, index=False)\n","                first_write = False\n","                time.sleep(0.5)  # Optional delay\n","\n","    print(f\"Done. Responses saved to: {output_csv}\")\n"],"metadata":{"id":"LQAPbmpIuowm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["filename = \"\" #name of the file containing the test data generated by Step 1\n","\n","get_responses(filename)"],"metadata":{"id":"qpBx94zqu-dM"},"execution_count":null,"outputs":[]}]}