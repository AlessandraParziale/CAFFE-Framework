{"cells":[{"cell_type":"markdown","metadata":{"id":"rQ5P8a1PmrtT"},"source":["# **Research Questions**"]},{"cell_type":"markdown","metadata":{"id":"RQ2uCFwVRzZO"},"source":["##**RQ1 - Entropy** (Optimal number of counterfactual prompts to be generated by Step 1 of CAFFE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Za-FR0bZdhg_","collapsed":true},"outputs":[],"source":["import math\n","import re\n","from collections import Counter\n","from typing import List, Tuple\n","import pandas as pd\n","\n","# -------------------------------------------------\n","TOKEN_RE = re.compile(r\"\\b\\w+\\b\", re.UNICODE)\n","\n","def tokenize(text: str) -> List[str]:\n","    return TOKEN_RE.findall(text.lower())\n","\n","# -------------------------------------------------\n","# Shannon entropy with Miller–Madow\n","def entropy_mm(tokens: List[str]) -> float:\n","    N = len(tokens)\n","    if N == 0:\n","        return 0.0\n","    counts = Counter(tokens)\n","    V = len(counts)\n","    H_naive = -sum((c / N) * math.log2(c / N) for c in counts.values())\n","    H_mm = H_naive + (V - 1) / (2 * N * math.log(2))\n","    return H_mm\n","\n","# -------------------------------------------------\n","def find_plateau(\n","    sentences: List[str],\n","    batch_size: int = 10,\n","    epsilon: float = 0.02,\n","    k: int = 3\n",") -> Tuple[int, List[float]]:\n","\n","    tokens, h_curve = [], []\n","    below_threshold_run = 0\n","    stop_index = -1\n","\n","    for i in range(0, len(sentences), batch_size):\n","        batch = sentences[i : i + batch_size]\n","        for s in batch:\n","            tokens.extend(tokenize(s))\n","\n","        h_now = entropy_mm(tokens)\n","        h_curve.append(h_now)\n","\n","        if len(h_curve) > 1:\n","            delta = h_curve[-1] - h_curve[-2]\n","            if delta < epsilon:\n","                below_threshold_run += 1\n","                if below_threshold_run >= k and stop_index == -1:\n","                    stop_index = i + batch_size\n","            else:\n","                below_threshold_run = 0\n","\n","    return stop_index, h_curve\n","\n","\n","# -------------------------------------------------\n","def analyse_in_blocks(\n","    csv_path: str,\n","    block_size: int = 20,\n","    batch_size: int = 1,\n","    epsilon: float = 0.02,\n","    k: int = 3,\n",") -> pd.DataFrame:\n","    df = pd.read_csv(csv_path)\n","\n","    results = []\n","    for start in range(0, len(df), block_size):\n","        chunk = df.iloc[start : start + block_size]\n","\n","        if \"bias_type\" in chunk.columns and chunk[\"bias_type\"].notna().any():\n","            bias_mode = chunk[\"bias_type\"].mode().iloc[0]\n","        else:\n","            bias_mode = None\n","\n","        sentences = chunk[\"sentence\"].tolist()\n","        stop_at, curve = find_plateau(\n","            sentences, batch_size=batch_size, epsilon=epsilon, k=k\n","        )\n","\n","        results.append(\n","            {\n","                \"block_idx\": start // block_size + 1,\n","                \"rows\": f\"{start}-{start+len(chunk)-1}\",\n","                \"bias_type\": bias_mode,\n","                \"stop_at\": stop_at,\n","                \"final_entropy\": curve[-1],\n","                \"entropy_curve\": curve,\n","            }\n","        )\n","        print(f\"\\nBlock {results[-1]['block_idx']}  ({results[-1]['rows']})\")\n","        print(f\" bias_type: {bias_mode}\")\n","        print(\" Entropy-rate curve:\", curve)\n","        if stop_at != -1:\n","            print(f\" Plateau dopo {stop_at} frasi.\")\n","        else:\n","            print(\" Nessun plateau in questo blocco.\")\n","\n","    return pd.DataFrame(results)\n","\n","# -------------------------------------------------\n","if __name__ == \"__main__\":\n","    summary = analyse_in_blocks(\n","        \"RQ1_Joined_Prompts_for_Evaluation.csv\",\n","        block_size=20,\n","        batch_size=1,\n","        epsilon=0.02,\n","        k=3,\n","    )\n","\n","    summary.to_csv(\"entropy_summary.csv\", index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"52JA6dwDMJkw"},"source":["### **Summary of Mean - Median**"]},{"cell_type":"code","source":["import pandas as pd\n","\n","try:\n","    df = pd.read_csv('entropy_summary.csv', engine='python', on_bad_lines='skip')\n","except Exception as e:\n","    print(f\"Error reading CSV: {e}\")\n","    raise\n","\n","df['stop_at'] = pd.to_numeric(df['stop_at'], errors='coerce')\n","\n","stats = df.groupby('bias_type')['stop_at'].agg([\n","    ('mean_stop_at', 'mean'),\n","    ('median_stop_at', 'median'),\n","    ('std_stop_at', 'std'),\n","    ('count', 'count')\n","]).reset_index()\n","\n","output_path = 'RQ1_Results.csv'\n","stats.to_csv(output_path, index=False)\n","stats"],"metadata":{"id":"CHBqjA3rukfW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QgAu-wUkXeUM"},"source":["**We consider the median (12 pairs of prompts)**\n","\n","--------\n"]},{"cell_type":"markdown","metadata":{"id":"e4lAT_MElLaJ"},"source":["## **RQ2 - Responses + Similarity Metrics** (Best semantic similarity metric to be implemented in Step 3 of CAFFE)"]},{"cell_type":"markdown","metadata":{"id":"UQCLH6_BZdL1"},"source":["### **Responses**"]},{"cell_type":"markdown","source":["#### Identification of a statistically significant set for intent - bias\n"],"metadata":{"id":"HROG0Xq8DMLz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"isCSdqKXCTKG"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from math import ceil\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RfPMQA6fRt29"},"outputs":[],"source":["PATH = '/content/RQ3_TD_Test_Data.csv'\n","df   = pd.read_csv(PATH)\n","print(f\"Loaded {len(df):,} rows\")\n","\n","# === Build a pair index: every two consecutive rows share the same index ===\n","df['pair_idx']      = df.index // 2\n","df['pos_in_pair']   = df.groupby('pair_idx').cumcount() + 1\n","\n","\n","# === Pivot from long to wide: one row per pair ===\n","wide = (\n","    df.pivot(index='pair_idx',\n","             columns='pos_in_pair',\n","             values=['group', 'sentence'])\n","      .reset_index(drop=True)\n",")\n","\n","# === Flatten the MultiIndex columns that result from the pivot ===\n","wide.columns = [\n","    f'{col[0]}_{col[1]}'\n","    for col in wide.columns\n","]\n","\n","# Bring back the metadata columns (topic, intent, bias_type) – they are identical within each pair\n","meta_cols = ['topic', 'intent', 'bias_type']\n","meta = (\n","    df.groupby('pair_idx')[meta_cols]\n","      .first()\n","      .reset_index(drop=True)\n",")\n","\n","# Final wide DataFrame\n","wide_df = pd.concat([meta, wide], axis=1)\n","\n","print(f\"Wide table has {len(wide_df):,} rows and {wide_df.shape[1]} columns\")\n","wide_df.head()\n","\n","OUT_WIDE = '/content/RQ3_TD_Test_Data_Joined.csv'\n","wide_df.to_csv(OUT_WIDE, index=False)\n","print(f\"Wide version written to {OUT_WIDE}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_zamZ7GUdEU"},"outputs":[],"source":["PATH = '/content/sentence_pairs_wide.csv'\n","wide_df = pd.read_csv(PATH)\n","print(f\"{len(wide_df):,} rows (pairs) loaded\")\n","\n","# === Sample-size function (finite-population correction) ===\n","def sample_size(N, E=0.05, p=0.5, Z=1.96):\n","    \"\"\"\n","    N : population size (number of pairs in the set)\n","    E : margin of error (default 0.05 = 5 %)\n","    p : estimated proportion (0.5 = worst-case, largest n)\n","    Z : Z-score (1.96 → 95 % confidence)\n","    \"\"\"\n","    n0 = (Z**2 * p * (1 - p)) / (E**2)\n","    return int(ceil(n0 / (1 + (n0 - 1) / N)))\n","\n","# === Draw the samples ===\n","rng          = np.random.default_rng()\n","samples      = []\n","summary_rows = []\n","\n","for (bias, intent), grp in wide_df.groupby(['bias_type', 'intent'], sort=False):\n","    N        = len(grp)\n","    n_needed = sample_size(N)\n","    chosen   = rng.choice(grp.index,\n","                          size=min(N, n_needed),\n","                          replace=False)\n","    samples.append(wide_df.loc[chosen])\n","\n","    summary_rows.append({\n","        'bias_type'   : bias,\n","        'intent'      : intent,\n","        'total_pairs' : N,\n","        'sample_size' : n_needed,\n","        'drawn_pairs' : len(chosen)\n","    })\n","\n","sampled_df = pd.concat(samples, ignore_index=True)\n","summary_df = pd.DataFrame(summary_rows)\n","\n","# ---------------------\n","OUT_SAMPLE = '/content/sampled_pairs.csv'\n","OUT_STATS  = '/content/sample_sizes.csv'\n","\n","sampled_df.to_csv(OUT_SAMPLE, index=False)\n","summary_df.to_csv(OUT_STATS,  index=False)\n","\n","print(f\"{len(sampled_df):,} rows (pairs) written to {OUT_SAMPLE}\")\n","print(f\"Sample-size table written to {OUT_STATS}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w5LElov2VAno"},"outputs":[],"source":["# === Summary table ===\n","summary_df = pd.DataFrame(summary_rows, columns=[\n","    'bias_type', 'intent', 'total_pairs', 'sample_size', 'drawn_pairs'\n","])\n","\n","totals = summary_df[['total_pairs', 'sample_size', 'drawn_pairs']].sum()\n","total_row = pd.DataFrame([{\n","    'bias_type'  : 'TOTAL',\n","    'intent'     : '',\n","    'total_pairs': totals['total_pairs'],\n","    'sample_size': totals['sample_size'],\n","    'drawn_pairs': totals['drawn_pairs']\n","}])\n","\n","summary_df = pd.concat([summary_df, total_row], ignore_index=True)\n","\n","# === Result ===\n","print(summary_df)\n","print(\"\\nGrand totals –\")\n","print(\"  total_pairs :\", totals['total_pairs'])\n","print(\"  sample_size :\", totals['sample_size'])\n","print(\"  drawn_pairs :\", totals['drawn_pairs'])\n","\n","\n","OUT_STATS = '/content/sample_sizes.csv'\n","summary_df.to_csv(OUT_STATS, index=False)\n","print(f\"Updated sample-size table (with totals) saved to {OUT_STATS}\")"]},{"cell_type":"markdown","metadata":{"id":"kvCWrfumZ76g"},"source":["#### Responses Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-GCuFGKZaGH4"},"outputs":[],"source":["!pip install --quiet --upgrade openai tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ch0FRCt8aN-M"},"outputs":[],"source":["import os\n","from google.colab import userdata\n","from openai import OpenAI\n","import pandas as pd\n","from tqdm.auto import tqdm\n","import time\n","from google.colab import drive\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3ww2SOXbnIW"},"outputs":[],"source":["drive.mount(\"/content/drive\", force_remount=False)\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n","client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n","\n","drive_path  = \"/.../sampled_pairs.csv\" # Final File Save Path\n","\n","df = pd.read_csv(drive_path)\n","\n","required = {\"sentence_1\", \"sentence_2\"}\n","missing  = required - set(df.columns)\n","if missing:\n","    raise ValueError(f\"Missing required columns: {missing}\")\n","\n","for col in (\"response_1\", \"response_2\"):\n","    if col not in df.columns:\n","        df[col] = \"\"\n","\n","def gpt_reply(prompt: str, model: str = \"gpt-4o-mini\") -> str:\n","    try:\n","        res = client.chat.completions.create(\n","            model=model,\n","            messages=[{\"role\": \"user\", \"content\": prompt}],\n","        )\n","        return res.choices[0].message.content.strip()\n","    except Exception as e:\n","        print(f\"Errore con prompt «{prompt[:50]}…»: {e}\")\n","        return \"\"\n","\n","todo = (df[\"response_1\"] == \"\").sum() + (df[\"response_2\"] == \"\").sum()\n","\n","\n","with tqdm(total=todo, desc=\"Response Generation\") as pbar:\n","    for idx, row in df.iterrows():\n","        # sentence_1\n","        if row[\"response_1\"] == \"\":\n","            df.at[idx, \"response_1\"] = gpt_reply(row[\"sentence_1\"])\n","            pbar.update(1)\n","\n","        # sentence_2\n","        if row[\"response_2\"] == \"\":\n","            df.at[idx, \"response_2\"] = gpt_reply(row[\"sentence_2\"])\n","            pbar.update(1)\n","\n","        # Save after each row\n","        df.to_csv(drive_path, index=False)\n","        time.sleep(1)\n","\n","df.to_csv(drive_path, index=False)\n","print(\"Save completed\")"]},{"cell_type":"markdown","metadata":{"id":"LozSWVH3XmEc"},"source":["### **Similarity Metrics**"]},{"cell_type":"markdown","source":["#### BERT"],"metadata":{"id":"qDCpoBzRTyBh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XHFRwdpCRDC"},"outputs":[],"source":["!pip install transformers torch --quiet"]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertModel\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","from sklearn.metrics.pairwise import cosine_similarity\n","import pandas as pd"],"metadata":{"id":"gYNzBVunT3aE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('RQ2_Responses.csv')\n","\n","# Model and Tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","model = BertModel.from_pretrained('bert-base-uncased')\n","model.eval()\n","\n","# Embedding\n","def get_sentence_embedding(text):\n","    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128)\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    embeddings = outputs.last_hidden_state.squeeze(0)\n","    attention_mask = inputs['attention_mask'].squeeze(0)\n","    valid_embeddings = embeddings[attention_mask == 1]\n","\n","    mean_embedding = valid_embeddings.mean(dim=0).cpu().numpy()\n","    return mean_embedding\n","\n","# Cosine Similarity\n","def get_cosine_similarity(emb1, emb2):\n","    emb1 = emb1.reshape(1, -1)\n","    emb2 = emb2.reshape(1, -1)\n","    return cosine_similarity(emb1, emb2)[0][0]\n","\n","\n","similarities = []\n","for idx, row in tqdm(df.iterrows(), total=len(df)):\n","    text1 = row['response_1']\n","    text2 = row['response_2']\n","    emb1 = get_sentence_embedding(text1)\n","    emb2 = get_sentence_embedding(text2)\n","    sim = get_cosine_similarity(emb1, emb2)\n","    similarities.append(sim)\n","\n","df['BERT_similarity'] = similarities\n","df.to_csv('Responses_with_similarity.csv', index=False)"],"metadata":{"id":"FwCn3BH8UIyB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Latent Semantic Analysis (LSA)"],"metadata":{"id":"zcMjjv2-VZM2"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import TruncatedSVD\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"0-ZtQfzAVi1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = \"Responses_with_similarity.csv\"\n","df = pd.read_csv(path)\n","all_texts = df['response_1'].tolist() + df['response_2'].tolist()\n","\n","# TF-IDF\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(all_texts)\n","\n","# LSA\n","lsa = TruncatedSVD(n_components=100, random_state=42)\n","X_lsa = lsa.fit_transform(X)\n","\n","n = len(df)\n","X1, X2 = X_lsa[:n], X_lsa[n:]\n","\n","# Similarity\n","lsa_sims = [cosine_similarity([v1], [v2])[0][0] for v1, v2 in zip(X1, X2)]\n","\n","df['LSA_similarity'] = lsa_sims\n","df.to_csv(path, index=False)"],"metadata":{"id":"KK9KVVlzUOwK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Latent Dirichlet Allocation (LDA)"],"metadata":{"id":"qUt0-2Z8Vx3_"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"ySBkI1lBWLV7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = \"Responses_with_similarity.csv\"\n","df = pd.read_csv(path)\n","all_texts = df['response_1'].tolist() + df['response_2'].tolist()\n","\n","# Count matrix\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(all_texts)\n","\n","# LDA\n","lda = LatentDirichletAllocation(n_components=30, random_state=42)\n","X_lda = lda.fit_transform(X)\n","\n","\n","X1, X2 = X_lda[:n], X_lda[n:]\n","\n","# Similarity\n","lda_sims = [cosine_similarity([v1], [v2])[0][0] for v1, v2 in zip(X1, X2)]\n","\n","df['LDA_similarity'] = lda_sims\n","df.to_csv(path, index=False)"],"metadata":{"id":"ysfUo7pkVyLg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Data Analysis**"],"metadata":{"id":"nN1EYC4LVe1D"}},{"cell_type":"code","source":["!pip install pandas matplotlib seaborn --quiet"],"metadata":{"id":"FiGgoviNVgZ8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"UUJKdxdCVnNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = \"RQ2_Responses_with_similarity.csv\"\n","df = pd.read_csv(path)\n","\n","# Convert similarity columns to numeric\n","similarity_cols = ['BERT_similarity', 'LSA_similarity', 'LDA_similarity']\n","df[similarity_cols] = df[similarity_cols].apply(pd.to_numeric, errors='coerce')\n","\n","# === Overall Statistics ===\n","overall_stats = df[similarity_cols].describe().T\n","print(\"=== Overall Similarity Statistics ===\")\n","print(overall_stats)\n","\n","# === Statistics Grouped by bias_type (flattened) ===\n","grouped = df.groupby('bias_type')[similarity_cols].describe()\n","\n","# === Flatten multi-level columns ===\n","grouped.columns = [f'{col}_{stat}' for col, stat in grouped.columns]\n","\n","# ===Reset index to move bias_type back as a column ===\n","grouped_stats = grouped.reset_index()\n","\n","print(\"\\n=== Cleaned Grouped Statistics (one row per bias_type) ===\")\n","print(grouped_stats)\n","\n","# === Visualizations ===\n","\n","# === Boxplot of similarity scores by bias type ===\n","plt.figure(figsize=(12, 6))\n","for i, sim in enumerate(similarity_cols):\n","    plt.subplot(1, 3, i+1)\n","    sns.boxplot(data=df, x='bias_type', y=sim)\n","    plt.title(f'{sim} by Bias Type')\n","    plt.xticks(rotation=45)\n","plt.tight_layout()\n","plt.show()\n","\n","# === Histogram of each similarity metric ===\n","for sim in similarity_cols:\n","    plt.figure(figsize=(6, 4))\n","    sns.histplot(df[sim], bins=20, kde=True)\n","    plt.title(f'Distribution of {sim}')\n","    plt.xlabel(sim)\n","    plt.ylabel('Frequency')\n","    plt.show()"],"metadata":{"id":"3zCYX6PrV8ns","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Ensure similarity columns are numeric ===\n","similarity_cols = ['BERT_similarity', 'LSA_similarity', 'LDA_similarity']\n","df[similarity_cols] = df[similarity_cols].apply(pd.to_numeric, errors='coerce')\n","\n","# === Set thresholds from 0.1 to 0.9 ===\n","thresholds = np.arange(0.1, 1.0, 0.1)\n","\n","# === Function to classify PASS/FAIL per row and threshold ===\n","def evaluate_row(row, threshold):\n","    return {\n","        metric: \"PASS\" if row[metric] >= threshold else \"FAIL\"\n","        for metric in similarity_cols\n","    }\n","\n","# === Store results ===\n","results_all = []\n","results_by_bias = []\n","\n","# === Evaluate each row at each threshold ===\n","for t in thresholds:\n","    df_temp = df.copy()\n","    df_temp[[f\"{col}_result_{t:.1f}\" for col in similarity_cols]] = df.apply(\n","        lambda row: pd.Series(evaluate_row(row, t)), axis=1\n","    )\n","    df_temp['threshold'] = t\n","    results_all.append(df_temp)\n","\n","\n","results_all_df = pd.concat(results_all, ignore_index=True)\n","\n","#===  Melt to long format ===\n","melted = pd.melt(\n","    results_all_df,\n","    id_vars=['threshold', 'bias_type'],\n","    value_vars=[f\"{col}_result_{t:.1f}\" for t in thresholds for col in similarity_cols],\n","    var_name=\"metric_threshold\",\n","    value_name=\"result\"\n",")\n","\n","# === Extract original metric and threshold ===\n","melted['metric'] = melted['metric_threshold'].apply(lambda x: x.split('_result_')[0])\n","melted['used_threshold'] = melted['metric_threshold'].apply(lambda x: float(x.split('_result_')[1]))\n","\n","# === Overall PASS/FAIL statistics ===\n","summary_all = (\n","    melted.groupby(['metric', 'used_threshold', 'result'])\n","    .size()\n","    .unstack(fill_value=0)\n","    .reset_index()\n",")\n","\n","# === Grouped PASS/FAIL by bias_type ===\n","summary_by_bias = (\n","    melted.groupby(['bias_type', 'metric', 'used_threshold', 'result'])\n","    .size()\n","    .unstack(fill_value=0)\n","    .reset_index()\n",")\n","\n","print(summary_all)\n","print(summary_by_bias)"],"metadata":{"id":"AWr3w7y1osJC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Count fairness bugs (FAILs) globally per metric–threshold ===\n","df_global = (\n","    melted[melted['result'] == 'FAIL']\n","    .groupby(['metric', 'used_threshold'])\n","    .size()\n","    .reset_index(name='fairness_bugs')\n",")\n","\n","# === Count fairness bugs (FAILs) per bias_type ===\n","df_bias = (\n","    melted[melted['result'] == 'FAIL']\n","    .groupby(['bias_type', 'metric', 'used_threshold'])\n","    .size()\n","    .reset_index(name='fairness_bugs')\n",")\n","\n","# === Identify the best overall metric–threshold combo ===\n","best_global = df_global.sort_values(by='fairness_bugs', ascending=False).iloc[0]\n","\n","print(\"BEST METRIC–THRESHOLD COMBINATION (OVERALL):\")\n","print(f\"- Metric: {best_global['metric']}\")\n","print(f\"- Threshold: {best_global['used_threshold']}\")\n","print(f\"- Fairness Bugs Detected: {best_global['fairness_bugs']}\\n\")\n","\n","# === Best metric–threshold per bias_type ===\n","best_per_bias = (\n","    df_bias.sort_values(by='fairness_bugs', ascending=False)\n","    .groupby('bias_type', as_index=False)\n","    .first()\n",")\n","\n","print(\"BEST COMBINATION BY BIAS TYPE:\")\n","for _, row in best_per_bias.iterrows():\n","    print(f\"- {row['bias_type']}: {row['metric']} @ {row['used_threshold']} (bugs: {row['fairness_bugs']})\")\n"],"metadata":{"id":"SjIlLihMpVNK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Count FAILs per metric–threshold ===\n","df_global = (\n","    melted[melted['result'] == 'FAIL']\n","    .groupby(['metric', 'used_threshold'])\n","    .size()\n","    .reset_index(name='fairness_bugs')\n",")\n","\n","# === Count FAILs per bias_type–metric–threshold ===\n","df_bias = (\n","    melted[melted['result'] == 'FAIL']\n","    .groupby(['bias_type', 'metric', 'used_threshold'])\n","    .size()\n","    .reset_index(name='fairness_bugs')\n",")\n","\n","# === Correct total cases ===\n","num_test_cases = df.shape[0]\n","\n","# For global: each metric–threshold combo has exactly num_test_cases\n","df_total_global = pd.DataFrame([\n","    {\n","        'metric': metric,\n","        'used_threshold': threshold,\n","        'total_cases': num_test_cases\n","    }\n","    for metric in similarity_cols\n","    for threshold in thresholds\n","])\n","\n","# For bias-specific: base counts only on original df\n","df_total_bias_base = df.groupby('bias_type').size().reset_index(name='base_cases')\n","\n","df_total_bias = pd.DataFrame([\n","    {\n","        'bias_type': row['bias_type'],\n","        'metric': metric,\n","        'used_threshold': threshold,\n","        'total_cases': row['base_cases']\n","    }\n","    for _, row in df_total_bias_base.iterrows()\n","    for metric in similarity_cols\n","    for threshold in thresholds\n","])\n","\n","# === Merge and compute fail rates ===\n","df_global = pd.merge(df_global, df_total_global, on=['metric', 'used_threshold'])\n","df_global['fail_rate'] = df_global['fairness_bugs'] / df_global['total_cases'] * 100\n","\n","df_bias = pd.merge(df_bias, df_total_bias, on=['bias_type', 'metric', 'used_threshold'])\n","df_bias['fail_rate'] = df_bias['fairness_bugs'] / df_bias['total_cases'] * 100\n","\n","# === Identify best overall combination ===\n","global_mean_fail_rate = df_global['fail_rate'].mean()\n","df_global['fail_rate_diff_from_mean'] = df_global['fail_rate'] - global_mean_fail_rate\n","best_global = df_global.sort_values(by='fail_rate_diff_from_mean', ascending=False).iloc[0]\n","\n","print(\"BEST METRIC–THRESHOLD COMBINATION (RELATIVE TO MEAN FAIL RATE):\")\n","print(f\"- Metric: {best_global['metric']}\")\n","print(f\"- Threshold: {best_global['used_threshold']}\")\n","print(f\"- Fairness Bugs: {best_global['fairness_bugs']} out of {best_global['total_cases']}\")\n","print(f\"- FAIL Rate: {best_global['fail_rate']:.2f}% (Δ from mean: {best_global['fail_rate_diff_from_mean']:.2f}%)\\n\")\n","\n","# === Best per bias type ===\n","best_per_bias = (\n","    df_bias.sort_values(by='fail_rate', ascending=False)\n","    .groupby('bias_type', as_index=False)\n","    .first()\n",")\n","\n","print(\"BEST COMBINATION BY BIAS TYPE:\")\n","for _, row in best_per_bias.iterrows():\n","    print(f\"- {row['bias_type']}: {row['metric']} @ {row['used_threshold']} → \"\n","          f\"{row['fairness_bugs']} FAILs out of {row['total_cases']} \"\n","          f\"({row['fail_rate']:.2f}%)\")\n","\n","\n","df_global.to_csv(\"global_fairness_bug_rates.csv\", index=False)\n","df_bias.to_csv(\"bias_specific_fairness_bug_rates.csv\", index=False)\n","best_per_bias.to_csv(\"best_combinations_per_bias.csv\", index=False)\n","\n","from google.colab import files\n","files.download(\"global_fairness_bug_rates.csv\")\n","files.download(\"bias_specific_fairness_bug_rates.csv\")\n","files.download(\"best_combinations_per_bias.csv\")\n","\n","# === Print top 10 globally ranked configs ===\n","print(\"\\n TOP 10 METRIC–THRESHOLD COMBINATIONS BY FAIL RATE:\")\n","top_10 = df_global.sort_values(by='fail_rate', ascending=False).head(10)\n","print(top_10[['metric', 'used_threshold', 'fail_rate', 'fairness_bugs', 'total_cases']].to_string(index=False))\n"],"metadata":{"id":"vahV1FonqDUC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["--------------------------\n"],"metadata":{"id":"pBDqq9XWBMV2"}},{"cell_type":"markdown","source":["## **RQ3 - Overall Evaluation** (Evaluation of CAFFE results against METAL)"],"metadata":{"id":"WotJmA5K9ix9"}},{"cell_type":"code","source":["import pandas as pd\n","\n","path = \"Result_LLAMA_Responses_RQ3_TD_Test_Data_Joined.csv\"\n","df = pd.read_csv(path)\n","\n","# === Compute overall ASR and similarity stats ===\n","total_execs = len(df)\n","unsatisfied = (df['ResultLabel'] == 'FAIL').sum()\n","asr_overall = unsatisfied / total_execs\n","\n","overall_mean = df['ActualResult'].mean()\n","overall_median = df['ActualResult'].median()\n","overall_std = df['ActualResult'].std()\n","\n","# === Compute per bias_type ASR and similarity stats ===\n","summary_by_bias = df.groupby('bias_type').agg(\n","    Total_Executions=('ResultLabel', 'count'),\n","    Fails=('ResultLabel', lambda x: (x == 'FAIL').sum()),\n","    Mean=('ActualResult', 'mean'),\n","    Median=('ActualResult', 'median'),\n","    StdDev=('ActualResult', 'std')\n",").reset_index()\n","\n","summary_by_bias['ASR'] = summary_by_bias['Fails'] / summary_by_bias['Total_Executions']\n","\n","# === Append overall row ===\n","overall_row = pd.DataFrame([{\n","    'bias_type': 'Overall',\n","    'Total_Executions': total_execs,\n","    'Fails': unsatisfied,\n","    'Mean': overall_mean,\n","    'Median': overall_median,\n","    'StdDev': overall_std,\n","    'ASR': asr_overall\n","}])\n","\n","summary_with_overall = pd.concat([summary_by_bias, overall_row], ignore_index=True)\n","\n","summary_with_overall.to_csv(\"RQ3_Results_\" + path, index=False)\n","print(summary_with_overall.to_string(index=False))\n","\n","\n"],"metadata":{"id":"KZrSQF539ov7"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1rMcleeW3GrPIvQiqmWFYdCYusMla5r67","timestamp":1751356195872}],"collapsed_sections":["RQ2uCFwVRzZO","52JA6dwDMJkw","e4lAT_MElLaJ","UQCLH6_BZdL1","HROG0Xq8DMLz","kvCWrfumZ76g","LozSWVH3XmEc","qDCpoBzRTyBh","zcMjjv2-VZM2","qUt0-2Z8Vx3_","nN1EYC4LVe1D","WotJmA5K9ix9"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}